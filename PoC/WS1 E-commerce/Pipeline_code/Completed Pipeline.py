# Workstream 1: Olist Data Engineering Pipeline (PoC)
#H·ª£p nh·∫•t 9 t·ªáp d·ªØ li·ªáu Olist th√†nh m·ªôt B·∫£ng D·ªØ li·ªáu T·ªïng th·ªÉ (Master Table) duy nh·∫•t v√† tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng nghi·ªáp v·ª• (features) v√† Ph√¢n t√≠ch l√†m s·∫°ch d·ªØ li·ªáu v·ªõi c√°c quy tr√¨nh ti√™u chu·∫©n


"""
WORKSTREAM 1 (OLIST) - PIPELINE HO√ÄN CH·ªàNH (T·ª™ NOTEBOOKS)

M·ª•c ƒë√≠ch:
1.  T·∫£i (Load) c√°c t·ªáp .csv c·ªßa Olist.
2.  H·ª£p nh·∫•t (Merge) ch√∫ng m·ªôt c√°ch an to√†n (x·ª≠ l√Ω b·∫´y 'payments').
3.  T·∫°o (Create) c√°c ƒë·∫∑c tr∆∞ng nghi·ªáp v·ª• (features) bao g·ªìm geolocation/distance.
4.  L√†m s·∫°ch (Clean) & ƒêi·ªÅn Nulls (Impute) t·∫≠p trung ·ªü cu·ªëi.
5.  Ki·ªÉm tra (Validate) chi ti·∫øt ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu cu·ªëi c√πng.
6.  Xu·∫•t (Save) ra m·ªôt file CSV cu·ªëi c√πng ƒë√£ l√†m s·∫°ch.

C√°ch ch·∫°y (t·ª´ Terminal):
> pip install pandas numpy haversine pyarrow
> python pipeline_ws1_final.py
"""

import pandas as pd
import numpy as np
import os
import sys
import time
from haversine import haversine # C·∫ßn c√†i ƒë·∫∑t: pip install haversine
import logging # S·ª≠ d·ª•ng logging thay cho print ƒë·ªÉ qu·∫£n l√Ω t·ªët h∆°n
import json
import pprint

# C·∫•u h√¨nh Logging c∆° b·∫£n
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# C·∫•u h√¨nh Pandas
pd.set_option('display.max_columns', None)
pd.set_option('display.float_format', lambda x: '%.2f' % x)
pd.options.mode.chained_assignment = None # T·∫Øt c·∫£nh b√°o CopyWarning (ch·ªâ d√πng n·∫øu hi·ªÉu r√µ code)

# --- 1. H√ÄM T·∫¢I D·ªÆ LI·ªÜU ---

def load_data(data_dir='data/'):
    """T·∫£i t·∫•t c·∫£ c√°c t·ªáp CSV c·∫ßn thi·∫øt v√†o m·ªôt dictionary c·ªßa DataFrames."""
    logging.info(f"[B∆∞·ªõc 1/7] ƒêang t·∫£i d·ªØ li·ªáu t·ª´ th∆∞ m·ª•c: {data_dir}...")
    files_to_keys = {
        'olist_orders_dataset.csv': 'orders', 'olist_order_items_dataset.csv': 'items',
        'olist_products_dataset.csv': 'products', 'olist_customers_dataset.csv': 'customers',
        'olist_order_reviews_dataset.csv': 'reviews', 'olist_order_payments_dataset.csv': 'payments',
        'olist_sellers_dataset.csv': 'sellers', 'olist_geolocation_dataset.csv': 'geolocation',
        'product_category_name_translation.csv': 'translation' # Th√™m file translation
    }
    dataframes = {}
    try:
        for file, key in files_to_keys.items():
            file_path = os.path.join(data_dir, file)
            dataframes[key] = pd.read_csv(file_path)
        logging.info(f"-> T·∫£i {len(dataframes)} t·ªáp d·ªØ li·ªáu ch√≠nh th√†nh c√¥ng.")
        logging.info(f"-> C√°c kh√≥a (keys) ƒë√£ t·∫°o: {list(dataframes.keys())}")
        return dataframes
    except FileNotFoundError as e:
        logging.error(f"üö® L·ªñI: Kh√¥ng t√¨m th·∫•y file {e.filename}. ƒê·∫£m b·∫£o c√°c t·ªáp CSV n·∫±m trong th∆∞ m·ª•c '{data_dir}'.")
        sys.exit(1)

def aggregate_payments(df_payments):
    """(QUAN TR·ªåNG) X·ª≠ l√Ω "B·∫´y H·ª£p nh·∫•t" üí£. G·ªôp b·∫£ng payments."""
    logging.info("[B∆∞·ªõc 2/7] ƒêang g·ªôp (Aggregate) b·∫£ng 'payments'...")
    df_payments_agg = df_payments.groupby('order_id').agg(
        payment_installments_total=('payment_installments', 'sum'),
        payment_value_total=('payment_value', 'sum'),
        payment_type_primary=('payment_type', 'first'),
        payment_sequential_count=('payment_sequential', 'max') # Th√™m t·ª´ Notebook 1
    ).reset_index()
    logging.info(f"-> ƒê√£ g·ªôp 'payments' t·ª´ {len(df_payments)} h√†ng xu·ªëng {len(df_payments_agg)} h√†ng.")
    return df_payments_agg

def aggregate_geolocation(df_geo):
    """Aggregate geolocation ƒë·ªÉ t·ªëi ∆∞u merge."""
    logging.info("[B∆∞·ªõc 3/7] ƒêang g·ªôp (Aggregate) b·∫£ng 'geolocation'...")
    # L·∫•y t·ªça ƒë·ªô trung b√¨nh cho m·ªói zip code
    df_geo_agg = df_geo.groupby('geolocation_zip_code_prefix').agg(
        geo_lat=('geolocation_lat', 'mean'),
        geo_lng=('geolocation_lng', 'mean')
    ).reset_index()
    logging.info(f"-> ƒê√£ g·ªôp 'geolocation' t·ª´ {len(df_geo)} h√†ng xu·ªëng {len(df_geo_agg)} h√†ng (zip codes duy nh·∫•t).")
    return df_geo_agg

# --- 2. H√ÄM H·ª¢P NH·∫§T ---

def merge_tables(dataframes, df_payments_agg, df_geo_agg):
    """Th·ª±c thi pipeline h·ª£p nh·∫•t (merge) c√°c b·∫£ng."""
    logging.info("[B∆∞·ªõc 4/7] ƒêang h·ª£p nh·∫•t (Merge) c√°c b·∫£ng...")
    df_master = dataframes['orders'].copy()

    # Merge b·∫£ng ch√≠nh
    df_master = pd.merge(df_master, dataframes['customers'], on='customer_id', how='left')
    df_reviews_dedup = dataframes['reviews'].sort_values('review_creation_date', ascending=False).drop_duplicates('order_id', keep='first')
    df_master = pd.merge(df_master, df_reviews_dedup, on='order_id', how='left')
    df_master = pd.merge(df_master, df_payments_agg, on='order_id', how='left')
    df_master = pd.merge(df_master, dataframes['items'], on='order_id', how='left')
    df_master = pd.merge(df_master, dataframes['products'], on='product_id', how='left')
    df_master = pd.merge(df_master, dataframes['sellers'], on='seller_id', how='left')
    df_master = pd.merge(df_master, dataframes['translation'], on='product_category_name', how='left')

    # Merge Geolocation (2 l·∫ßn, ƒë√£ aggregate)
    # L·∫ßn 1: Customer
    df_master = pd.merge(df_master, df_geo_agg, left_on='customer_zip_code_prefix', right_on='geolocation_zip_code_prefix', how='left')
    df_master.rename(columns={'geo_lat': 'customer_lat', 'geo_lng': 'customer_lng'}, inplace=True)
    df_master.drop(columns=['geolocation_zip_code_prefix'], inplace=True, errors='ignore')

    # L·∫ßn 2: Seller
    df_master = pd.merge(df_master, df_geo_agg, left_on='seller_zip_code_prefix', right_on='geolocation_zip_code_prefix', how='left', suffixes=('', '_seller_geo'))
    df_master.rename(columns={'geo_lat': 'seller_lat', 'geo_lng': 'seller_lng'}, inplace=True)
    df_master.drop(columns=['geolocation_zip_code_prefix_seller_geo', 'geolocation_zip_code_prefix'], inplace=True, errors='ignore') # X√≥a c·∫£ 2 c·ªôt zip th·ª´a

    logging.info(f"-> H·ª£p nh·∫•t (Merge) th√†nh c√¥ng. K√≠ch th∆∞·ªõc b·∫£ng t·ªïng th·ªÉ: {df_master.shape}")
    return df_master

# --- 3. H√ÄM T·∫†O ƒê·∫∂C TR∆ØNG (CH∆ØA CLEAN) ---

def create_features(df_merged):
    """T·∫°o t·∫•t c·∫£ c√°c ƒë·∫∑c tr∆∞ng nghi·ªáp v·ª•."""
    logging.info("[B∆∞·ªõc 5/7] ƒêang t·∫°o ƒë·∫∑c tr∆∞ng (Feature Engineering)...")
    df_featured = df_merged.copy()

    # 1. Chuy·ªÉn ƒë·ªïi Th·ªùi gian
    time_cols = ['order_purchase_timestamp', 'order_approved_at',
                 'order_delivered_carrier_date', 'order_delivered_customer_date',
                 'order_estimated_delivery_date', 'shipping_limit_date',
                 'review_creation_date', 'review_answer_timestamp']
    for col in time_cols:
        if col in df_featured.columns: # Ki·ªÉm tra tr∆∞·ªõc khi chuy·ªÉn ƒë·ªïi
            df_featured[col] = pd.to_datetime(df_featured[col], errors='coerce')

    # 2. ƒê·∫∑c tr∆∞ng V·∫≠n h√†nh
    if 'order_delivered_customer_date' in df_featured.columns and 'order_purchase_timestamp' in df_featured.columns:
        df_featured['delivery_time_days'] = (df_featured['order_delivered_customer_date'] - df_featured['order_purchase_timestamp']).dt.total_seconds() / (24 * 60 * 60)
    if 'order_estimated_delivery_date' in df_featured.columns and 'order_delivered_customer_date' in df_featured.columns:
        df_featured['delivery_vs_estimated_days'] = (df_featured['order_estimated_delivery_date'] - df_featured['order_delivered_customer_date']).dt.total_seconds() / (24 * 60 * 60)
    if 'order_delivered_carrier_date' in df_featured.columns and 'order_purchase_timestamp' in df_featured.columns:
        df_featured['order_processing_time_days'] = (df_featured['order_delivered_carrier_date'] - df_featured['order_purchase_timestamp']).dt.total_seconds() / (24 * 60 * 60)

    # 3. ƒê·∫∑c tr∆∞ng Cyclical Time (T·ª´ Notebook 1)
    if 'order_purchase_timestamp' in df_featured.columns:
        df_featured['purchase_year'] = df_featured['order_purchase_timestamp'].dt.year
        df_featured['purchase_month'] = df_featured['order_purchase_timestamp'].dt.month
        df_featured['purchase_day_of_week'] = df_featured['order_purchase_timestamp'].dt.dayofweek # 0=Mon, 6=Sun
        df_featured['purchase_hour'] = df_featured['order_purchase_timestamp'].dt.hour
        df_featured['is_weekend'] = df_featured['purchase_day_of_week'].apply(lambda x: 1 if x >= 5 else 0)

    # 4. ƒê·∫∑c tr∆∞ng ƒê·ªãa l√Ω (Kho·∫£ng c√°ch)
    if 'customer_lat' in df_featured.columns and 'seller_lat' in df_featured.columns:
        locations_available = df_featured[['customer_lat', 'customer_lng', 'seller_lat', 'seller_lng']].notnull().all(axis=1)
        distances = df_featured[locations_available].apply(
            lambda row: haversine((row['customer_lat'], row['customer_lng']), (row['seller_lat'], row['seller_lng'])),
            axis=1
        )
        df_featured['dist_cust_seller_km'] = np.nan
        df_featured.loc[locations_available, 'dist_cust_seller_km'] = distances
        logging.info(" -> ƒê√£ t√≠nh 'dist_cust_seller_km'.")

    # 5. ƒê·∫∑c tr∆∞ng T√†i ch√≠nh & S·∫£n ph·∫©m
    if 'price' in df_featured.columns and 'freight_value' in df_featured.columns:
        df_featured['freight_ratio'] = df_featured['freight_value'] / (df_featured['price'] + 1e-6)
        df_featured['freight_ratio'] = df_featured['freight_ratio'].replace([np.inf, -np.inf], 0) # X·ª≠ l√Ω inf
    if 'product_length_cm' in df_featured.columns: # Ki·ªÉm tra t·ªìn t·∫°i
        df_featured['product_volume_cm3'] = (
            df_featured['product_length_cm'] * df_featured['product_height_cm'] * df_featured['product_width_cm']
        )
        logging.info(" -> ƒê√£ t√≠nh 'freight_ratio' v√† 'product_volume_cm3'.")

    # 6. ƒê·∫∑c tr∆∞ng Thanh to√°n (C·ªù)
    if 'payment_type_primary' in df_featured.columns:
        df_featured['is_payment_credit_card'] = (df_featured['payment_type_primary'] == 'credit_card').astype(float)
        df_featured['is_payment_boleto'] = (df_featured['payment_type_primary'] == 'boleto').astype(float)
        df_featured['is_payment_voucher'] = (df_featured['payment_type_primary'] == 'voucher').astype(float)
    if 'payment_installments_total' in df_featured.columns:
        df_featured['is_payment_installments'] = (df_featured['payment_installments_total'] > 1).astype(float)

    # 7. (T√ôY CH·ªåN) ƒê·∫∑c tr∆∞ng Review Time-Safe (T·ªëi ∆∞u 3) - C√≥ th·ªÉ th√™m h√†m fix_review_leakage ·ªü ƒë√¢y

    logging.info(f"-> T·∫°o ƒë·∫∑c tr∆∞ng ho√†n t·∫•t. Shape hi·ªán t·∫°i: {df_featured.shape}")
    return df_featured

# --- 4. H√ÄM L√ÄM S·∫†CH & ƒêI·ªÄN NULLS CU·ªêI C√ôNG ---

def clean_and_impute(df_featured):
    """L√†m s·∫°ch v√† ƒëi·ªÅn T·∫§T C·∫¢ nulls c√≤n l·∫°i M·ªòT L·∫¶N."""
    logging.info("[B∆∞·ªõc 6/7] ƒêang th·ª±c hi·ªán l√†m s·∫°ch cu·ªëi c√πng v√† ƒëi·ªÅn Nulls...")
    df_clean = df_featured.copy()

    # === 1. L√ÄM S·∫†CH (Cleaning) ===
    # 1.1 Cardinality (Category Name)
    cat_cols_to_clean = ['product_category_name', 'product_category_name_english']
    for col in cat_cols_to_clean:
        if col in df_clean.columns:
            df_clean[col] = df_clean[col].str.lower().str.strip()

    # 1.2 Outliers & Logic (D·ª±a tr√™n Notebook 2)
    # Cap Freight Ratio
    if 'freight_ratio' in df_clean.columns:
        df_clean['freight_ratio'] = df_clean['freight_ratio'].clip(upper=10)
    # X·ª≠ l√Ω gi√° tr·ªã √¢m (tr·ª´ sentinel -999)
    cols_non_negative = ['price', 'freight_value', 'payment_value_total']
    for col in cols_non_negative:
        if col in df_clean.columns:
            df_clean.loc[df_clean[col] < 0, col] = 0
    delivery_negative_mask = (df_clean['delivery_time_days'] < 0) & (df_clean['delivery_time_days'] != -999)
    if delivery_negative_mask.any():
        df_clean.loc[delivery_negative_mask, 'delivery_time_days'] = 0

    # === 2. ƒêI·ªÄN NULLS (Imputation) ===
    # Chi·∫øn l∆∞·ª£c ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a ·ªü ƒë√¢y

    # 2.1 C·ªôt Review Score (0 = Ch∆∞a review)
    if 'review_score' in df_clean.columns:
        df_clean['review_score'] = df_clean['review_score'].fillna(0)

    # 2.2 C·ªôt V·∫≠n h√†nh (Ch∆∞a giao = -999)
    delivery_cols_to_flag = ['delivery_time_days', 'delivery_vs_estimated_days', 'order_processing_time_days']
    for col in delivery_cols_to_flag:
        if col in df_clean.columns:
            df_clean[col] = df_clean[col].fillna(-999)

    # 2.3 C·ªôt Ph√¢n lo·∫°i (Categorical = 'unknown')
    categorical_cols_to_unknown = [
        'product_category_name', 'product_category_name_english',
        'payment_type_primary',
        'customer_city', 'customer_state',
        'seller_city', 'seller_state'
    ]
    for col in categorical_cols_to_unknown:
        if col in df_clean.columns:
            df_clean[col] = df_clean[col].fillna('unknown')

    # 2.4 C·ªôt S·ªë h·ªçc (Numeric = 0 ho·∫∑c median/mean n·∫øu h·ª£p l√Ω)
    # ƒêi·ªÅn 0 cho c√°c gi√° tr·ªã n√†y
    numeric_cols_to_zero = [
        'payment_installments_total', 'payment_value_total',
        'price', 'freight_value', 'freight_ratio',
        'product_name_lenght', 'product_description_lenght', 'product_photos_qty',
        'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm',
        'product_volume_cm3',
        'is_payment_credit_card', 'is_payment_boleto', 'is_payment_voucher',
        'is_payment_installments', 'payment_sequential_count' # Th√™m payment_sequential_count
    ]
    for col in numeric_cols_to_zero:
        if col in df_clean.columns:
            df_clean[col] = df_clean[col].fillna(0)

    # ƒêi·ªÅn mean cho Kho·∫£ng c√°ch (n·∫øu thi·∫øu lat/lng)
    if 'dist_cust_seller_km' in df_clean.columns:
         mean_dist = df_clean['dist_cust_seller_km'].mean() # T√≠nh mean tr√™n c·ªôt ƒë√£ t√≠nh (lo·∫°i b·ªè NaN)
         df_clean['dist_cust_seller_km'] = df_clean['dist_cust_seller_km'].fillna(mean_dist if not pd.isna(mean_dist) else 0)

    # ƒêi·ªÅn 0 cho c√°c c·ªôt Lat/Lng c√≤n s√≥t (sau merge geo)
    geo_coords = ['customer_lat', 'customer_lng', 'seller_lat', 'seller_lng']
    for col in geo_coords:
        if col in df_clean.columns:
            df_clean[col] = df_clean[col].fillna(0)

    # Chuy·ªÉn c√°c c·ªôt c·ªù v·ªÅ int sau khi fillna
    flag_cols = ['is_payment_credit_card', 'is_payment_boleto', 'is_payment_voucher', 'is_payment_installments', 'is_weekend']
    for col in flag_cols:
        if col in df_clean.columns:
            df_clean[col] = df_clean[col].astype(int)

    # 2.5 C√°c c·ªôt kh√°c (Review text/dates, approved_at, etc.) - B·ªè qua ho·∫∑c ƒëi·ªÅn n·∫øu c·∫ßn
    # V√≠ d·ª•:
    # df_clean['review_comment_message'] = df_clean['review_comment_message'].fillna('none')

    # 3. L√†m s·∫°ch cu·ªëi c√πng (lo·∫°i b·ªè h√†ng thi·∫øu kh√≥a ch√≠nh)
    df_clean.dropna(subset=['order_id', 'order_item_id'], inplace=True)

    logging.info("-> L√†m s·∫°ch cu·ªëi c√πng v√† ƒëi·ªÅn Nulls ho√†n t·∫•t.")
    return df_clean


# --- 5. H√ÄM KI·ªÇM TRA (VALIDATION FUNCTION) ---
# S·ª≠ d·ª•ng h√†m comprehensive_validation chi ti·∫øt t·ª´ Notebook 2
def comprehensive_validation(df, verbose=True):
    """Validation t·ªïng h·ª£p to√†n di·ªán (l·∫•y t·ª´ Notebook 2)."""
    logging.info("[B∆∞·ªõc 7/7] ƒêang ki·ªÉm tra (Validate) pipeline cu·ªëi c√πng...")  # C·∫≠p nh·∫≠t s·ªë b∆∞·ªõc
    validation_results = {}
    issues_found = False  # C·ªù ƒë·ªÉ theo d√µi l·ªói

    # 3.1: Th√¥ng tin c∆° b·∫£n
    if verbose: logging.info("\n--- 3.1 Th√¥ng tin c∆° b·∫£n DataFrame ---")
    validation_results['shape'] = df.shape
    validation_results['memory_mb'] = round(df.memory_usage(deep=True).sum() / 1024 ** 2, 2)
    if verbose:
        logging.info(f"‚úì Shape: {validation_results['shape']}")
        logging.info(f"‚úì Memory: {validation_results['memory_mb']} MB")

    # 3.2: Ki·ªÉm tra Missing Values
    if verbose: logging.info("\n--- 3.2 Ki·ªÉm tra Missing Values ---")

    # ƒê·ªãnh nghƒ©a c√°c c·ªôt ƒë∆∞·ª£c ph√©p l√† Null/NaT (v√¨ ch√∫ng mang √Ω nghƒ©a nghi·ªáp v·ª•)
    cols_allowed_to_be_null = [
        'review_comment_title',
        'review_comment_message',
        'order_approved_at',
        'order_delivered_carrier_date',
        'order_delivered_customer_date',
        'shipping_limit_date',
        'review_creation_date',
        'review_answer_timestamp',
        'review_id'
    ]
    # T·∫°o danh s√°ch c√°c c·ªôt C·∫¶N PH·∫¢I S·∫†CH (kh√¥ng ƒë∆∞·ª£c Null)
    all_cols = df.columns.tolist()
    cols_to_validate = [col for col in all_cols if col not in cols_allowed_to_be_null]

    # T·∫°o m·ªôt DataFrame t·∫°m th·ªùi CH·ªà ch·ª©a c√°c c·ªôt c·∫ßn ki·ªÉm tra
    df_validate = df[cols_to_validate]
    # --- [S·ª¨A ƒê·ªîI K·∫æT TH√öC] ---

    # S·ª≠a c√°c d√≤ng t√≠nh to√°n ƒë·ªÉ d√πng df_validate, kh√¥ng d√πng df
    total_cells = df_validate.shape[0] * df_validate.shape[1]
    total_missing = df_validate.isna().sum().sum()  # S·ª¨A ·ªû ƒê√ÇY
    missing_pct = round(total_missing * 100.0 / total_cells, 2) if total_cells > 0 else 0
    validation_results['total_missing_values'] = total_missing
    validation_results['missing_pct'] = missing_pct
    validation_results['cols_with_missing'] = df_validate.isna().any().sum()  # S·ª¨A ·ªû ƒê√ÇY

    if total_missing > 0:
        # C·∫≠p nh·∫≠t th√¥ng b√°o l·ªói
        logging.warning(
            f"-> üö® KI·ªÇM TRA MISSING TH·∫§T B·∫†I: V·∫´n c√≤n {total_missing:,} gi√° tr·ªã Null ({missing_pct}%) TRONG C√ÅC C·ªòT QUAN TR·ªåNG.")
        issues_found = True
        if verbose:
            top_missing = df_validate.isna().sum().sort_values(ascending=False).head(5)  # S·ª¨A ·ªû ƒê√ÇY
            top_missing = top_missing[top_missing > 0]
            logging.warning("  Top c·ªôt (quan tr·ªçng) thi·∫øu nhi·ªÅu nh·∫•t:")
            for col, count in top_missing.items():
                pct = round(count * 100.0 / df_validate.shape[0], 2)  # S·ª¨A ·ªû ƒê√ÇY
                logging.warning(f"    - {col}: {count:,} ({pct}%)")
    elif verbose:
        logging.info("‚úì Ki·ªÉm tra Missing: ƒê·∫°t.")

    # 3.3: Ki·ªÉm tra Duplicates (To√†n b·ªô h√†ng)
    if verbose: logging.info("\n--- 3.3 Ki·ªÉm tra Duplicates ---")
    dup_rows = df.duplicated().sum()
    validation_results['duplicate_rows'] = dup_rows
    if dup_rows > 0:
        dup_pct = round(dup_rows * 100.0 / df.shape[0], 2)
        logging.warning(f"-> üö® KI·ªÇM TRA DUPLICATES TH·∫§T B·∫†I: T√¨m th·∫•y {dup_rows:,} h√†ng tr√πng l·∫∑p ({dup_pct}%).")
        issues_found = True
    elif verbose:
        logging.info("‚úì Ki·ªÉm tra Duplicates: ƒê·∫°t.")

    # 3.4: Ki·ªÉm tra Key Integrity (Granularity)
    if verbose: logging.info("\n--- 3.4 Ki·ªÉm tra Key Integrity (Granularity) ---")
    key_cols = ['order_id', 'order_item_id']
    if all(col in df.columns for col in key_cols):
        df_check = df.copy()
        df_check[key_cols[0]] = df_check[key_cols[0]].fillna('MISSING_ORDER')
        df_check[key_cols[1]] = df_check[key_cols[1]].fillna('MISSING_ITEM')
        dup_keys = df_check.duplicated(subset=key_cols).sum()
        validation_results['duplicate_keys'] = dup_keys
        if dup_keys > 0:
            logging.warning(f"-> üö® KI·ªÇM TRA KEY TH·∫§T B·∫†I: T√¨m th·∫•y {dup_keys:,} h√†ng tr√πng l·∫∑p theo kh√≥a {key_cols}.")
            issues_found = True
        elif verbose:
            logging.info(f"‚úì Ki·ªÉm tra Key Integrity {key_cols}: ƒê·∫°t.")
    else:
        logging.error(f"-> üö® KI·ªÇM TRA KEY TH·∫§T B·∫†I: Thi·∫øu c·ªôt kh√≥a {key_cols}.")
        issues_found = True
        validation_results['duplicate_keys'] = -1  # Indicate check failed

    # 3.5: Ki·ªÉm tra Business Logic Constraints
    if verbose: logging.info("\n--- 3.5 Ki·ªÉm tra Business Logic Constraints ---")
    violations = {}
    if 'review_score' in df.columns:
        invalid_reviews = df[(df['review_score'] < 0) | (df['review_score'] > 5)]
        violations['invalid_review_score'] = len(invalid_reviews)
    if 'price' in df.columns: violations['negative_price'] = (df['price'] < 0).sum()  # Ch·ªâ c·∫ßn < 0 v√¨ 0 c√≥ th·ªÉ h·ª£p l·ªá
    if 'freight_value' in df.columns: violations['negative_freight'] = (df['freight_value'] < 0).sum()
    if 'delivery_time_days' in df.columns:
        invalid_delivery = (df['delivery_time_days'] < -999).sum()  # Ch·ªâ ki·ªÉm tra < -999
        violations['invalid_delivery_time'] = invalid_delivery
    total_violations = sum(violations.values())
    validation_results['business_logic_violations'] = violations
    if total_violations > 0:
        logging.warning(f"-> üö® KI·ªÇM TRA LOGIC TH·∫§T B·∫†I: T√¨m th·∫•y {total_violations} vi ph·∫°m logic nghi·ªáp v·ª•.")
        issues_found = True
        if verbose: print(violations)
    elif verbose:
        logging.info("‚úì Ki·ªÉm tra Business Logic: ƒê·∫°t.")

    # 3.6: Validation Score
    score = 100.0
    score -= min(missing_pct * 5, 25)  # missing_pct b√¢y gi·ªù ƒë√£ ƒë∆∞·ª£c t√≠nh ƒë√∫ng
    score -= min((dup_rows * 100.0 / df.shape[0]) * 5, 15) if df.shape[0] > 0 else 0
    violation_pct = total_violations * 100.0 / df.shape[0] if df.shape[0] > 0 else 0
    score -= min(violation_pct * 10, 20)
    validation_results['quality_score'] = round(max(score, 0), 2)

    if verbose:
        logging.info(f"\n--- 3.6 Overall Data Quality Score ---")
        logging.info(f"üéØ Quality Score: {validation_results['quality_score']}/100")
        if validation_results['quality_score'] >= 90:
            logging.info("‚úÖ EXCELLENT")
        elif validation_results['quality_score'] >= 75:
            logging.info("‚úì GOOD")
        else:
            logging.warning("‚ö† FAIR/POOR")

    validation_results['passed'] = not issues_found
    return validation_results

# --- 6. H√ÄM CH√çNH (MAIN FUNCTION) ---

def main():
    """ƒêi·ªÅu ph·ªëi to√†n b·ªô pipeline."""
    start_time = time.time()
    DATA_DIR = 'data/'
    OUTPUT_FILE_CSV = 'olist_master_table_final.csv'  # ƒê·ªïi t√™n file output cu·ªëi
    OUTPUT_FILE_PARQUET = 'olist_master_table_final.parquet'
    # --- Ch·∫°y Pipeline ---
    dataframes = load_data(DATA_DIR)
    df_payments_agg = aggregate_payments(dataframes['payments'])
    df_geo_agg = aggregate_geolocation(dataframes['geolocation']) # Th√™m b∆∞·ªõc aggregate geo
    df_merged = merge_tables(dataframes, df_payments_agg, df_geo_agg) # Truy·ªÅn df_geo_agg v√†o
    df_featured = create_features(df_merged)
    # df_featured = fix_review_leakage(df_featured) # T·∫°m th·ªùi comment T·ªëi ∆∞u 3 n·∫øu ch∆∞a c·∫ßn
    df_final = clean_and_impute(df_featured) # B∆∞·ªõc l√†m s·∫°ch v√† impute cu·ªëi c√πng

    # --- Ki·ªÉm tra & L∆∞u ---
    validation_report = comprehensive_validation(df_final, verbose=True)  # Ch·∫°y validation chi ti·∫øt

    # Ch√∫ng ta s·∫Ω kh√¥ng ch·∫∑n vi·ªác l∆∞u file n·∫øu validation th·∫•t b·∫°i,
    # nh∆∞ng ch√∫ng ta s·∫Ω ghi l·∫°i c·∫£nh b√°o v√† v·∫´n l∆∞u file l·ªói.

    if not validation_report['passed']:
        logging.warning("\n‚ö†Ô∏è C·∫¢NH B√ÅO: Pipeline kh√¥ng v∆∞·ª£t qua ki·ªÉm tra, nh∆∞ng V·∫™N TI·∫æN H√ÄNH L∆ØU FILE.")
        # V·∫´n gi·ªØ l·∫°i ph·∫ßn l∆∞u file JSON b√°o l·ªói t·ª´ kh·ªëi 'else' c≈©
        try:
            validation_file_error = 'validation_report_FAILED.json'
            clean_report = {}
            for k, v in validation_report.items():
                try:
                    json.dumps({k: v})
                    clean_report[k] = v
                except TypeError:
                    clean_report[k] = str(v)

            with open(validation_file_error, 'w', encoding='utf-8') as f:
                json.dump(clean_report, f, indent=2, ensure_ascii=False)
            logging.info(f"-> ƒê√£ l∆∞u chi ti·∫øt l·ªói validation v√†o: {validation_file_error}")
        except Exception as json_e:
            logging.error(f"-> Kh√¥ng th·ªÉ l∆∞u validation report l·ªói: {json_e}")

    # KH·ªêI L∆ØU FILE N√ÄY GI·ªú S·∫º LU√îN CH·∫†Y (ƒë∆∞·ª£c ƒë∆∞a ra kh·ªèi 'if')
    logging.info(f"\n[B∆∞·ªõc 8/8] ƒêang l∆∞u tr·ªØ file {OUTPUT_FILE_CSV}...")

    try:
        # Ch·ªçn c√°c c·ªôt cu·ªëi c√πng ƒë·ªÉ l∆∞u
        final_columns = [
            'order_id', 'order_item_id', 'product_id', 'customer_id', 'seller_id',
            'order_purchase_timestamp',
            'delivery_time_days', 'delivery_vs_estimated_days', 'order_processing_time_days',
            'price', 'freight_value', 'freight_ratio',
            'is_payment_credit_card', 'is_payment_boleto', 'is_payment_voucher', 'is_payment_installments',
            'payment_value_total', 'payment_installments_total', 'payment_sequential_count',
            'review_score',
            'dist_cust_seller_km',
            'product_category_name', 'product_category_name_english',
            'customer_state', 'seller_state',
            'customer_lat', 'customer_lng', 'seller_lat', 'seller_lng',
            'product_weight_g', 'product_volume_cm3',
            'purchase_year', 'purchase_month', 'purchase_day_of_week', 'purchase_hour', 'is_weekend'
        ]
        final_columns_exist = [col for col in final_columns if col in df_final.columns]
        df_final_output = df_final[final_columns_exist]

        df_final_output.to_csv(OUTPUT_FILE_CSV, index=False)
        logging.info(f"\n--- ü•≥ HO√ÄN TH√ÄNH WORKSTREAM 1 (FINAL VERSION) ---")
        logging.info(f"Output ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {OUTPUT_FILE_CSV}")
        logging.info(f"K√≠ch th∆∞·ªõc cu·ªëi c√πng: {df_final_output.shape}")

    except Exception as e:
        logging.error(f"\nüö® L·ªñI khi l∆∞u file CSV: {e}")

    end_time = time.time()
    logging.info(f"\nT·ªïng th·ªùi gian ch·∫°y pipeline: {end_time - start_time:.2f} gi√¢y.")

# --- ƒêI·ªÇM B·∫ÆT ƒê·∫¶U CH·∫†Y SCRIPT ---
if __name__ == "__main__":
    main()
