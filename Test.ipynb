{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ducanh0405/datastorm/blob/main/Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YWZZc83t1J2",
        "outputId": "9d235310-8006-40f9-97ab-9fa6577cb73d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ƒê·ªçc file 'advanced_features_dataset.csv' th√†nh c√¥ng!\n",
            "\n",
            "‚è≥ B·∫Øt ƒë·∫ßu m√£ h√≥a c√°c c·ªôt ph√¢n lo·∫°i...\n",
            "‚úÖ M√£ h√≥a th√†nh c√¥ng!\n",
            "S·ªë c·ªôt sau khi m√£ h√≥a: 19\n",
            "\n",
            "‚è≥ B·∫Øt ƒë·∫ßu ph√¢n chia d·ªØ li·ªáu...\n",
            "‚úÖ Ph√¢n chia th√†nh c√¥ng!\n",
            "K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán (train): 35089 d√≤ng\n",
            "K√≠ch th∆∞·ªõc t·∫≠p ki·ªÉm tra (test): 8773 d√≤ng\n",
            "\n",
            "‚è≥ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh Random Forest...\n",
            "‚úÖ Hu·∫•n luy·ªán th√†nh c√¥ng!\n",
            "\n",
            "‚è≥ B·∫Øt ƒë·∫ßu ƒë√°nh gi√° hi·ªáu su·∫•t m√¥ h√¨nh...\n",
            "\n",
            "--- K·∫æT QU·∫¢ ƒê√ÅNH GI√Å ---\n",
            "Mean Absolute Error (MAE): 4.64 ph√∫t\n",
            "Ch·ªâ s·ªë R-squared (R¬≤): 0.59\n",
            "\n",
            "--- Di·ªÖn gi·∫£i ---\n",
            "-> MAE: Trung b√¨nh, m√¥ h√¨nh d·ª± ƒëo√°n sai l·ªách kho·∫£ng 4.64 ph√∫t so v·ªõi th·ªùi gian th·ª±c t·∫ø.\n",
            "-> R¬≤: M√¥ h√¨nh gi·∫£i th√≠ch ƒë∆∞·ª£c kho·∫£ng 59% s·ª± bi·∫øn ƒë·ªông c·ªßa th·ªùi gian giao h√†ng.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# --- B∆Ø·ªöC 0: T·∫¢I V√Ä XEM L·∫†I D·ªÆ LI·ªÜU ---\n",
        "try:\n",
        "    # Load the advanced features dataset which includes the 'rating_category'\n",
        "    df = pd.read_csv('/content/processed_dataset.csv')\n",
        "    print(\"‚úÖ ƒê·ªçc file 'advanced_features_dataset.csv' th√†nh c√¥ng!\")\n",
        "\n",
        "    # Remove columns not needed for training\n",
        "    # ID and Delivery_person_ID are not features for learning\n",
        "    # Also remove original date/time columns as cyclical features are created\n",
        "    df_model = df.drop(columns=['ID', 'Delivery_person_ID', 'Order_Date', 'Time_Orderd', 'Time_Order_picked'])\n",
        "\n",
        "    # --- B∆Ø·ªöC 1: M√É H√ìA (ENCODING) D·ªÆ LI·ªÜU CH·ªÆ ---\n",
        "    print(\"\\n‚è≥ B·∫Øt ƒë·∫ßu m√£ h√≥a c√°c c·ªôt ph√¢n lo·∫°i...\")\n",
        "\n",
        "    # Get list of columns to encode, including the new 'rating_category'\n",
        "    categorical_cols = [\n",
        "        'Weatherconditions',\n",
        "        'Road_traffic_density',\n",
        "        'Type_of_order',\n",
        "        'Festival',\n",
        "        'City',\n",
        "        'rating_category', # Include the binned rating category\n",
        "        'traffic_time_interaction' # Include the new interaction feature\n",
        "    ]\n",
        "\n",
        "    # Perform One-Hot Encoding\n",
        "    # Check if columns exist before encoding\n",
        "    cols_to_encode_exist = [col for col in categorical_cols if col in df_model.columns]\n",
        "    df_encoded = pd.get_dummies(df_model, columns=cols_to_encode_exist, drop_first=True)\n",
        "\n",
        "    print(\"‚úÖ M√£ h√≥a th√†nh c√¥ng!\")\n",
        "    print(f\"S·ªë c·ªôt sau khi m√£ h√≥a: {df_encoded.shape[1]}\")\n",
        "\n",
        "    # --- B∆Ø·ªöC 2: PH√ÇN CHIA D·ªÆ LI·ªÜU (TRAIN-TEST SPLIT) ---\n",
        "    print(\"\\n‚è≥ B·∫Øt ƒë·∫ßu ph√¢n chia d·ªØ li·ªáu...\")\n",
        "\n",
        "    # 'X' is all feature columns (input data)\n",
        "    X = df_encoded.drop('Time_taken (min)', axis=1)\n",
        "\n",
        "    # 'y' is the target column we want to predict\n",
        "    y = df_encoded['Time_taken (min)']\n",
        "\n",
        "    # Split data: 80% for training, 20% for testing\n",
        "    # random_state=42 to ensure the split is the same every time\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(\"‚úÖ Ph√¢n chia th√†nh c√¥ng!\")\n",
        "    print(f\"K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán (train): {X_train.shape[0]} d√≤ng\")\n",
        "    print(f\"K√≠ch th∆∞·ªõc t·∫≠p ki·ªÉm tra (test): {X_test.shape[0]} d√≤ng\")\n",
        "\n",
        "    # --- B∆Ø·ªöC 3: L·ª∞A CH·ªåN V√Ä HU·∫§N LUY·ªÜN M√î H√åNH ---\n",
        "    print(\"\\n‚è≥ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh Random Forest...\")\n",
        "\n",
        "    # Initialize the model\n",
        "    # n_estimators=100: the model will create 100 \"decision trees\"\n",
        "    # n_jobs=-1: use all available CPUs for faster training\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "    # \"Teach\" the model using training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    print(\"‚úÖ Hu·∫•n luy·ªán th√†nh c√¥ng!\")\n",
        "\n",
        "    # --- B∆Ø·ªöC 4: ƒê√ÅNH GI√Å M√î H√åNH ---\n",
        "    print(\"\\n‚è≥ B·∫Øt ƒë·∫ßu ƒë√°nh gi√° hi·ªáu su·∫•t m√¥ h√¨nh...\")\n",
        "\n",
        "    # Ask the model to predict on the test set (data it hasn't seen before)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Measure error\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(\"\\n--- K·∫æT QU·∫¢ ƒê√ÅNH GI√Å ---\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.2f} ph√∫t\")\n",
        "    print(f\"Ch·ªâ s·ªë R-squared (R¬≤): {r2:.2f}\")\n",
        "    print(\"\\n--- Di·ªÖn gi·∫£i ---\")\n",
        "    print(f\"-> MAE: Trung b√¨nh, m√¥ h√¨nh d·ª± ƒëo√°n sai l·ªách kho·∫£ng {mae:.2f} ph√∫t so v·ªõi th·ªùi gian th·ª±c t·∫ø.\")\n",
        "    print(f\"-> R¬≤: M√¥ h√¨nh gi·∫£i th√≠ch ƒë∆∞·ª£c kho·∫£ng {r2:.0%} s·ª± bi·∫øn ƒë·ªông c·ªßa th·ªùi gian giao h√†ng.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"\\n‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu! Vui l√≤ng ƒë·∫£m b·∫£o file 'advanced_features_dataset.csv' t·ªìn t·∫°i.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå ƒê√£ c√≥ l·ªói x·∫£y ra: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# --- B∆Ø·ªöC 0: T·∫¢I V√Ä XEM L·∫†I D·ªÆ LI·ªÜU ---\n",
        "try:\n",
        "    # Load the advanced features dataset which includes the 'rating_category'\n",
        "    df = pd.read_csv('/content/advanced_features_dataset.csv')\n",
        "    print(\"‚úÖ ƒê·ªçc file 'advanced_features_dataset.csv' th√†nh c√¥ng!\")\n",
        "\n",
        "    # Remove columns not needed for training\n",
        "    # ID and Delivery_person_ID are not features for learning\n",
        "    # Also remove original date/time columns as cyclical features are created\n",
        "    df_model = df.drop(columns=['ID', 'Delivery_person_ID', 'Order_Date', 'Time_Orderd', 'Time_Order_picked'])\n",
        "\n",
        "    # --- B∆Ø·ªöC 1: M√É H√ìA (ENCODING) D·ªÆ LI·ªÜU CH·ªÆ ---\n",
        "    print(\"\\n‚è≥ B·∫Øt ƒë·∫ßu m√£ h√≥a c√°c c·ªôt ph√¢n lo·∫°i...\")\n",
        "\n",
        "    # Get list of columns to encode, including the new 'rating_category'\n",
        "    categorical_cols = [\n",
        "        'Weatherconditions',\n",
        "        'Road_traffic_density',\n",
        "        'Type_of_order',\n",
        "        'Festival',\n",
        "        'City',\n",
        "        'rating_category', # Include the binned rating category\n",
        "        'traffic_time_interaction' # Include the new interaction feature\n",
        "    ]\n",
        "\n",
        "    # Perform One-Hot Encoding\n",
        "    # Check if columns exist before encoding\n",
        "    cols_to_encode_exist = [col for col in categorical_cols if col in df_model.columns]\n",
        "    df_encoded = pd.get_dummies(df_model, columns=cols_to_encode_exist, drop_first=True)\n",
        "\n",
        "    print(\"‚úÖ M√£ h√≥a th√†nh c√¥ng!\")\n",
        "    print(f\"S·ªë c·ªôt sau khi m√£ h√≥a: {df_encoded.shape[1]}\")\n",
        "\n",
        "    # --- B∆Ø·ªöC 2: PH√ÇN CHIA D·ªÆ LI·ªÜU (TRAIN-TEST SPLIT) ---\n",
        "    print(\"\\n‚è≥ B·∫Øt ƒë·∫ßu ph√¢n chia d·ªØ li·ªáu...\")\n",
        "\n",
        "    # 'X' is all feature columns (input data)\n",
        "    X = df_encoded.drop('Time_taken (min)', axis=1)\n",
        "\n",
        "    # 'y' is the target column we want to predict\n",
        "    y = df_encoded['Time_taken (min)']\n",
        "\n",
        "    # Split data: 80% for training, 20% for testing\n",
        "    # random_state=42 to ensure the split is the same every time\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(\"‚úÖ Ph√¢n chia th√†nh c√¥ng!\")\n",
        "    print(f\"K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán (train): {X_train.shape[0]} d√≤ng\")\n",
        "    print(f\"K√≠ch th∆∞·ªõc t·∫≠p ki·ªÉm tra (test): {X_test.shape[0]} d√≤ng\")\n",
        "\n",
        "    # --- B∆Ø·ªöC 3: L·ª∞A CH·ªåN V√Ä HU·∫§N LUY·ªÜN M√î H√åNH ---\n",
        "    print(\"\\n‚è≥ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh Random Forest...\")\n",
        "\n",
        "    # Initialize the model\n",
        "    # n_estimators=100: the model will create 100 \"decision trees\"\n",
        "    # n_jobs=-1: use all available CPUs for faster training\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "    # \"Teach\" the model using training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    print(\"‚úÖ Hu·∫•n luy·ªán th√†nh c√¥ng!\")\n",
        "\n",
        "    # --- B∆Ø·ªöC 4: ƒê√ÅNH GI√Å M√î H√åNH ---\n",
        "    print(\"\\n‚è≥ B·∫Øt ƒë·∫ßu ƒë√°nh gi√° hi·ªáu su·∫•t m√¥ h√¨nh...\")\n",
        "\n",
        "    # Ask the model to predict on the test set (data it hasn't seen before)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Measure error\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(\"\\n--- K·∫æT QU·∫¢ ƒê√ÅNH GI√Å ---\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.2f} ph√∫t\")\n",
        "    print(f\"Ch·ªâ s·ªë R-squared (R¬≤): {r2:.2f}\")\n",
        "    print(\"\\n--- Di·ªÖn gi·∫£i ---\")\n",
        "    print(f\"-> MAE: Trung b√¨nh, m√¥ h√¨nh d·ª± ƒëo√°n sai l·ªách kho·∫£ng {mae:.2f} ph√∫t so v·ªõi th·ªùi gian th·ª±c t·∫ø.\")\n",
        "    print(f\"-> R¬≤: M√¥ h√¨nh gi·∫£i th√≠ch ƒë∆∞·ª£c kho·∫£ng {r2:.0%} s·ª± bi·∫øn ƒë·ªông c·ªßa th·ªùi gian giao h√†ng.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"\\n‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu! Vui l√≤ng ƒë·∫£m b·∫£o file 'advanced_features_dataset.csv' t·ªìn t·∫°i.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå ƒê√£ c√≥ l·ªói x·∫£y ra: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAUQ6ImTRobd",
        "outputId": "933ee733-081d-4757-e10a-e890bde41ae3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ƒê·ªçc file 'advanced_features_dataset.csv' th√†nh c√¥ng!\n",
            "\n",
            "‚è≥ B·∫Øt ƒë·∫ßu m√£ h√≥a c√°c c·ªôt ph√¢n lo·∫°i...\n",
            "‚úÖ M√£ h√≥a th√†nh c√¥ng!\n",
            "S·ªë c·ªôt sau khi m√£ h√≥a: 29\n",
            "\n",
            "‚è≥ B·∫Øt ƒë·∫ßu ph√¢n chia d·ªØ li·ªáu...\n",
            "‚úÖ Ph√¢n chia th√†nh c√¥ng!\n",
            "K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán (train): 35089 d√≤ng\n",
            "K√≠ch th∆∞·ªõc t·∫≠p ki·ªÉm tra (test): 8773 d√≤ng\n",
            "\n",
            "‚è≥ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh Random Forest...\n",
            "‚úÖ Hu·∫•n luy·ªán th√†nh c√¥ng!\n",
            "\n",
            "‚è≥ B·∫Øt ƒë·∫ßu ƒë√°nh gi√° hi·ªáu su·∫•t m√¥ h√¨nh...\n",
            "\n",
            "--- K·∫æT QU·∫¢ ƒê√ÅNH GI√Å ---\n",
            "Mean Absolute Error (MAE): 4.70 ph√∫t\n",
            "Ch·ªâ s·ªë R-squared (R¬≤): 0.58\n",
            "\n",
            "--- Di·ªÖn gi·∫£i ---\n",
            "-> MAE: Trung b√¨nh, m√¥ h√¨nh d·ª± ƒëo√°n sai l·ªách kho·∫£ng 4.70 ph√∫t so v·ªõi th·ªùi gian th·ª±c t·∫ø.\n",
            "-> R¬≤: M√¥ h√¨nh gi·∫£i th√≠ch ƒë∆∞·ª£c kho·∫£ng 58% s·ª± bi·∫øn ƒë·ªông c·ªßa th·ªùi gian giao h√†ng.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "x·ª≠ l√Ω ƒë·ªÉ c·∫£i thi·ªán"
      ],
      "metadata": {
        "id": "x-nLEN65PZ2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- B∆Ø·ªö·ªöC 0: T·∫¢·∫¢I D·ªÆ LI·ªÜ·ªÜU ---\n",
        "try:\n",
        "    df = pd.read_csv('/content/processed_dataset.csv')\n",
        "    print(\"‚úÖ ƒê·ªçc file 'processed_dataset.csv' th√†nh c√¥ng!\")\n",
        "    print(f\"S·ªë d√≤ng ban ƒë·∫ßu: {df.shape[0]}, S·ªë c·ªôt ban ƒë·∫ßu: {df.shape[1]}\")\n",
        "\n",
        "    # --- B∆Ø·ªö·ªöC 1: PH√ÇN NH√ìM/R·ªúI R·∫†C H√ìA (BINNING) ---\n",
        "    print(\"\\n‚è≥ B·∫Øt ƒë·∫ßu Ph√¢n nh√≥m c·ªôt 'Delivery_person_Ratings'...\")\n",
        "\n",
        "    # ƒê·ªãnh nghƒ©a c√°c kho·∫£ng gi√° tr·ªã (bins) v√† nh√£n (labels)\n",
        "    bins = [0, 4.5, 4.8, 5.1]  # Kho·∫£ng gi√° tr·ªã: (0, 4.5], (4.5, 4.8], (4.8, 5.1]\n",
        "    labels = ['Rating_Thap', 'Rating_TrungBinh', 'Rating_Cao']\n",
        "\n",
        "    # T·∫°o c·ªôt m·ªõi 'rating_category'\n",
        "    df['rating_category'] = pd.cut(df['Delivery_person_Ratings'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "    print(\"‚úÖ Ph√¢n nh√≥m th√†nh c√¥ng!\")\n",
        "\n",
        "\n",
        "    # --- B∆Ø·ªö·ªöC 2: X·ª¨·ª¨ L√ù ƒê·∫∂·∫∂C TR∆Ø∆ØNG TU·∫¶·∫¶N HO√ÄN (CYCLICAL FEATURES) ---\n",
        "    print(\"\\n‚è≥ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω c√°c ƒë·∫∑c tr∆∞ng tu·∫ßn ho√†n...\")\n",
        "\n",
        "    # X·ª≠ l√Ω 'hour_of_day' (chu k·ª≥ 24 gi·ªù)\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['hour_of_day'] / 24.0)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['hour_of_day'] / 24.0)\n",
        "\n",
        "    # X·ª≠ l√Ω 'day_of_week' (chu k·ª≥ 7 ng√†y)\n",
        "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7.0)\n",
        "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7.0)\n",
        "\n",
        "    print(\"‚úÖ X·ª≠ l√Ω ƒë·∫∑c tr∆∞ng tu·∫ßn ho√†n th√†nh c√¥ng!\")\n",
        "\n",
        "\n",
        "    # --- B∆Ø·ªö·ªöC 3: T·∫†·∫†O ƒê·∫∂·∫∂C TR∆Ø∆ØNG T∆Ø∆Ø∆†NG T√ÅC (INTERACTION FEATURES) ---\n",
        "    print(\"\\n‚è≥ B·∫Øt ƒë·∫ßu t·∫°o ƒë·∫∑c tr∆∞ng t∆∞∆°ng t√°c...\")\n",
        "\n",
        "    # T·∫°o m·ªôt h√†m ƒë·ªÉ x√°c ƒë·ªãnh gi·ªù cao ƒëi·ªÉm\n",
        "    def is_peak_hour(hour):\n",
        "        # Gi·ªù ƒÉn tr∆∞a (11h-13h) v√† gi·ªù ƒÉn t·ªëi (18h-21h)\n",
        "        if (11 <= hour <= 13) or (18 <= hour <= 21):\n",
        "            return '_PeakHour'\n",
        "        else:\n",
        "            return '_OffPeakHour'\n",
        "\n",
        "    # T·∫°o ƒë·∫∑c tr∆∞ng t∆∞∆°ng t√°c gi·ªØa giao th√¥ng v√† gi·ªù cao ƒëi·ªÉm\n",
        "    df['traffic_time_interaction'] = df['Road_traffic_density'].str.strip() + df['hour_of_day'].apply(is_peak_hour)\n",
        "\n",
        "    print(\"‚úÖ T·∫°o ƒë·∫∑c tr∆∞ng t∆∞∆°ng t√°c th√†nh c√¥ng!\")\n",
        "\n",
        "\n",
        "    # --- B∆Ø·ªö·ªöC 4: D·ªå·ªåN D·∫∏·∫∏P V√Ä HO√ÄN THI·ªÜ·ªÜN ---\n",
        "    print(\"\\n‚è≥ D·ªçn d·∫πp c√°c c·ªôt g·ªëc...\")\n",
        "\n",
        "    # Lo·∫°i b·ªè c√°c c·ªôt g·ªëc ƒë√£ ƒë∆∞·ª£c bi·∫øn ƒë·ªïi\n",
        "    columns_to_drop = [\n",
        "        'Delivery_person_Ratings', # ƒê√£ ƒë∆∞·ª£c thay b·∫±ng 'rating_category'\n",
        "        'hour_of_day',             # ƒê√£ ƒë∆∞·ª£c thay b·∫±ng 'hour_sin' v√† 'hour_cos'\n",
        "        'day_of_week',             # ƒê√£ ƒë∆∞·ª£c thay b·∫±ng 'day_sin' v√† 'day_cos'\n",
        "        # Gi·ªØ l·∫°i 'Road_traffic_density' v√¨ n√≥ v·∫´n c√≥ gi√° tr·ªã\n",
        "    ]\n",
        "    df_advanced = df.drop(columns=columns_to_drop)\n",
        "\n",
        "    print(\"‚úÖ D·ªçn d·∫πp th√†nh c√¥ng!\")\n",
        "\n",
        "\n",
        "    # --- B∆Ø·ªö·ªöC 5: XU·∫§·∫§T FILE V√Ä XEM K·∫æ·∫æT QU·∫¢·∫¢ ---\n",
        "    output_filename = 'advanced_features_dataset.csv'\n",
        "    df_advanced.to_csv(output_filename, index=False)\n",
        "\n",
        "    print(f\"\\nüéâ TUY·ªÜT V·ªúI! ƒê√£ x·ª≠ l√Ω v√† xu·∫•t th√†nh c√¥ng file '{output_filename}'\")\n",
        "    print(\"--- Xem 5 d√≤ng ƒë·∫ßu c·ªßa file k·∫øt qu·∫£ v·ªõi c√°c c·ªôt m·ªõi: ---\")\n",
        "    print(df_advanced.head())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"\\n‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file 'processed_dataset.csv'! Vui l√≤ng t·∫£i file l√™n Colab tr∆∞·ªõc.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå ƒê√£ c√≥ l·ªói x·∫£y ra: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORjLVIIIPgEI",
        "outputId": "0d8bc728-5940-446c-ebc2-9cb5322cbe78"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ƒê·ªçc file 'processed_dataset.csv' th√†nh c√¥ng!\n",
            "S·ªë d√≤ng ban ƒë·∫ßu: 43862, S·ªë c·ªôt ban ƒë·∫ßu: 15\n",
            "\n",
            "‚è≥ B·∫Øt ƒë·∫ßu Ph√¢n nh√≥m c·ªôt 'Delivery_person_Ratings'...\n",
            "‚úÖ Ph√¢n nh√≥m th√†nh c√¥ng!\n",
            "\n",
            "‚è≥ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω c√°c ƒë·∫∑c tr∆∞ng tu·∫ßn ho√†n...\n",
            "‚úÖ X·ª≠ l√Ω ƒë·∫∑c tr∆∞ng tu·∫ßn ho√†n th√†nh c√¥ng!\n",
            "\n",
            "‚è≥ B·∫Øt ƒë·∫ßu t·∫°o ƒë·∫∑c tr∆∞ng t∆∞∆°ng t√°c...\n",
            "‚úÖ T·∫°o ƒë·∫∑c tr∆∞ng t∆∞∆°ng t√°c th√†nh c√¥ng!\n",
            "\n",
            "‚è≥ D·ªçn d·∫πp c√°c c·ªôt g·ªëc...\n",
            "‚úÖ D·ªçn d·∫πp th√†nh c√¥ng!\n",
            "\n",
            "üéâ TUY·ªÜT V·ªúI! ƒê√£ x·ª≠ l√Ω v√† xu·∫•t th√†nh c√¥ng file 'advanced_features_dataset.csv'\n",
            "--- Xem 5 d√≤ng ƒë·∫ßu c·ªßa file k·∫øt qu·∫£ v·ªõi c√°c c·ªôt m·ªõi: ---\n",
            "        ID Delivery_person_ID  Order_Date Time_Orderd Time_Order_picked  \\\n",
            "0  0x4607     INDORES13DEL02   2022-03-19    11:30:00          11:45:00   \n",
            "1  0xb379     BANGRES18DEL02   2022-03-25    19:45:00          19:50:00   \n",
            "2  0x5d6d     BANGRES19DEL01   2022-03-19    08:30:00          08:45:00   \n",
            "3  0x7a6a    COIMBRES13DEL02   2022-04-05    18:00:00          18:10:00   \n",
            "4  0x70a2     CHENRES12DEL01   2022-03-26    13:30:00          13:45:00   \n",
            "\n",
            "  Weatherconditions Road_traffic_density Type_of_order Festival  \\\n",
            "0             Sunny                 High        Snack       No    \n",
            "1            Stormy                  Jam        Snack       No    \n",
            "2        Sandstorms                  Low       Drinks       No    \n",
            "3             Sunny               Medium       Buffet       No    \n",
            "4            Cloudy                 High        Snack       No    \n",
            "\n",
            "             City  Time_taken (min)  distance_km   rating_category  hour_sin  \\\n",
            "0          Urban                 24     3.025149        Rating_Cao  0.258819   \n",
            "1  Metropolitian                 33    20.183530  Rating_TrungBinh -0.965926   \n",
            "2          Urban                 26     1.552758       Rating_Thap  0.866025   \n",
            "3  Metropolitian                 21     7.790401  Rating_TrungBinh -1.000000   \n",
            "4  Metropolitian                 30     6.210138  Rating_TrungBinh -0.258819   \n",
            "\n",
            "       hour_cos   day_sin   day_cos traffic_time_interaction  \n",
            "0 -9.659258e-01 -0.974928 -0.222521            High_PeakHour  \n",
            "1  2.588190e-01 -0.433884 -0.900969             Jam_PeakHour  \n",
            "2 -5.000000e-01 -0.974928 -0.222521          Low_OffPeakHour  \n",
            "3 -1.836970e-16  0.781831  0.623490          Medium_PeakHour  \n",
            "4 -9.659258e-01 -0.974928 -0.222521            High_PeakHour  \n"
          ]
        }
      ]
    }
  ]
}